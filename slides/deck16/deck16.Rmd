---
title: "Sta112FS <br> 15. Central Limit Theorem + CLT based inference, Pt. 1"
author: "Dr. Ã‡etinkaya-Rundel"
date: "November 3, 2015"
output:
  ioslides_presentation:
    highlight: pygments
    widescreen: yes
    css: ../lec.css
---

```{r set-options, echo=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(gridExtra)
```

```{r echo=FALSE}
qqline_params <- function(x){
  y <- quantile(x[!is.na(x)], c(0.25, 0.75))
  x <- qnorm(c(0.25, 0.75))
  slope <- diff(y) / diff(x)
  int <- y[1L] - slope * x[1L]
  return(list(slope = slope, int = int))
}
```


# Today's agenda

## Today's agenda {.smaller}

- Central Limit Theorem

  - Aside: Evaluating normality graphically
  - Application exercise: proving the CLT via simulation

- Inference based on the Central Limit Theorem

- **Due Thursday:** Read Sections 2.5 - 2.8 on OpenIntro: Intro Stat with 
Randomization and Simulation: http://www.openintro.org/isrs

# Notation

## Notation

- Means:
    - Population: mean = $\mu$, standard deviation = $\sigma$
    - Sample: mean = $\bar{x}$, standard deviation = $s$

- Proportions:
    - Population: $p$
    - Sample: $\hat{p}$
    
- Standard error: $SE$


# Central Limit Theorem

## Variability of sample statistics

- Each sample from the population yields a slightly different sample statistic 
(sample mean, sample proportion, etc.)

- The variability of these sample statistics is measured by the **standard error**

- Previously we quantified this value via simulation

- Today we talk about the theory underlying **sampling distributions**

## Sampling distribution

- **Sampling distribution** is the distribution of sample statistics of random
samples of size $n$ taken from a population

- In practice it is impossible to construct sampling distributions since it would 
require having access to the entire population

- Today for demonstration purposes we will assume we have access to the population
data, and construct sampling distributions, and examine their shapes, centers, and
spreads

# Evaluating normality: Normal probability plots

## Normal probability plot {.smaller}

```{r echo=FALSE}
set.seed(123)
```

```{r fig.width=9, fig.height=3.5}
d <- data.frame(norm_samp = rnorm(100, mean = 50, sd = 5))

ggplot(data = d, aes(sample = norm_samp)) +
  geom_point(alpha = 0.7, stat = "qq")
```

## Anatomy of a normal probability plot 
                            
- Data are plotted on the y-axis of a normal probability plot and theoretical 
quantiles (following a normal distribution) on the x-axis.

- If there is a one-to-one relationship between the data and the theoretical 
quantiles, then the data follow a nearly normal distribution.

- Since a one-to-one relationship would appear as a straight line on a scatter plot, 
the closer the points are to a perfect straight line, the more confident we can be 
that the data follow the normal model.

## Constructing a normal probability plot

Data (y-coordinates)| Percentile  | Theoretical Quantiles (x-coordinates)
------------- | -------------------------| -------------------------------
37.5          | 0.5 / 100 = 0.005     | `qnorm(0.005) = -2.58`
38.0          | 1.5 / 100 = 0.015     | `qnorm(0.015) = -2.17`
38.3          | 2.5 / 100 = 0.025     | `qnorm(0.025) = -1.95`
39.5          | 3.5 / 100 = 0.035     | `qnorm(0.035) = -1.81`
...           | ...                      | ...
61.9          | 99.5 / 100 = 0.995     | `qnorm(0.995) = 2.58`

## Constructing a normal probability plot {.smaller}

```{r fig.width=9, fig.height=4.5, echo=FALSE}
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", 
                "#0072B2", "#D55E00", "#CC79A7")

ggplot(data = d, aes(sample = norm_samp)) +
  geom_point(alpha = 0.7, stat = "qq") +
  geom_vline(xintercept = c(-2.58, -2.17, -1.95, -1.81, 2.58), linetype = "dashed",
             col = cbbPalette[3:7]) +
  geom_hline(yintercept = c(sort(d$norm_samp)[1:4], sort(d$norm_samp)[100]), 
             linetype = "dashed", col = cbbPalette[3:7])
```

## Fat tails

Best to think about what is happening with the most extreme values - here the 
biggest values are bigger than we would expect and the smallest values are smaller 
than we would expect (for a normal).

```{r fig.width=9, fig.height=3.5, echo=FALSE}
d <- data.frame(t = rt(1000, df = 5))

m <- mean(d$t)
s <- sd(d$t)

p1 <- ggplot(data = d, aes(x = t)) +
  geom_histogram(aes(y = ..density..), alpha = 0.7, binwidth = 0.5) +
  ylab("") +
  xlab("") +
  stat_function(fun = dnorm, args = list(mean = mean(d$t), sd = sd(d$t)))
p2 <- ggplot(data = d, aes(sample = t)) +
  geom_point(alpha = 0.7, stat = "qq") +
  geom_abline(intercept = qqline_params(d$t)$int, slope = qqline_params(d$t)$slope)
grid.arrange(p1, p2, ncol = 2)
```

## Skinny tails

Here the biggest values are smaller than we would expect and the smallest values are bigger than we would expect.

```{r fig.width=9, fig.height=3.5, echo=FALSE}
d <- data.frame(t = rnorm(1000))
d <- filter(d, t < 1.5 , t > -1.5)

m <- mean(d$t)
s <- sd(d$t)

p1 <- ggplot(data = d, aes(x = t)) +
  geom_histogram(aes(y = ..density..), alpha = 0.7, binwidth = 0.5) +
  xlim(c(m - 3.2*s, m + 3.2*s)) +
  ylab("") +
  xlab("") +
  stat_function(fun = dnorm, args = list(mean = mean(d$t), sd = sd(d$t)))
p2 <- ggplot(data = d, aes(sample = t)) +
  geom_point(alpha = 0.7, stat = "qq") +
  geom_abline(intercept = qqline_params(d$t)$int, slope = qqline_params(d$t)$slope)
grid.arrange(p1, p2, ncol = 2)
```

## Right skew

Here the biggest values are bigger than we would expect and the smallest values are also bigger than we would expect.

```{r fig.width=9, fig.height=3.5, echo=FALSE}
d <- data.frame(t = rlnorm(1000))
d <- filter(d, t < 10)

m <- mean(d$t)
s <- sd(d$t)

p1 <- ggplot(data = d, aes(x = t)) +
  geom_histogram(aes(y = ..density..), alpha = 0.7, binwidth = 0.5) +
  xlim(c(m - 3.2*s, m + 3.2*s)) +
  ylab("") +
  xlab("") +
  stat_function(fun = dnorm, args = list(mean = mean(d$t), sd = sd(d$t)))
p2 <- ggplot(data = d, aes(sample = t)) +
  geom_point(alpha = 0.7, stat = "qq") +
  geom_abline(intercept = qqline_params(d$t)$int, slope = qqline_params(d$t)$slope)
grid.arrange(p1, p2, ncol = 2)
```

## Left skew

Here the biggest values are smaller than we would expect and the smallest values are also smaller than we would expect.

```{r fig.width=9, fig.height=3.5, echo=FALSE}
d <- data.frame(t = rlnorm(1000))
d <- d %>%
  filter(t < 10) %>%
  mutate(t = 10 - t)


m <- mean(d$t)
s <- sd(d$t)

p1 <- ggplot(data = d, aes(x = t)) +
  geom_histogram(aes(y = ..density..), alpha = 0.7, binwidth = 0.5) +
  xlim(c(m - 3.2*s, m + 3.2*s)) +
  xlab("") +
  stat_function(fun = dnorm, args = list(mean = mean(d$t), sd = sd(d$t)))
p2 <- ggplot(data = d, aes(sample = t)) +
  geom_point(alpha = 0.7, stat = "qq") +
  geom_abline(intercept = qqline_params(d$t)$int, slope = qqline_params(d$t)$slope)
grid.arrange(p1, p2, ncol = 2)
```

# Back to sampling distributions

## Application exercise

See course website for details